# Lectura: Performance of recommender algorithms on top-n recommendation tasks. Cremonesi, P., Koren, Y., & Turrin, R. (2010). 

## Comentario

Este paper trata principalmente acerca de la manera en que se evalúan los sistemas recomendadores. En la práctica lo que se hace es separar el conjunto de datos en un set de *training* y uno de *testing*, donde a partir de los datos del entrenamiento (y un determinado modelo o enfoque) se intentan predecir los datos del *testing set*. Luego, típicamente se evalúan los resultados obtenidos a través de alguna métrica de error promedio (RMSE o MAE). Aquí es donde ocurre un problema pues lo que es de interés es evaluar qué tan buena es la **lista** recomendada y no el error promedio de los ratings de las predicciones. El paper analiza la importancia de esta diferencia comparando distintos modelos en el estado del arte. 

Previo a realizar las comparaciones se realiza una importante apreciación acerca del proceso de evaluación: la manera en que se construye el *testing set*. Cuando se intenta testear con películas populares con rating promedio muy bueno (5), es muy probable que el valor a predecir sea también 5 pues así se minimiza el error. Sin embargo, ¿qué se gana haciendo algo así si es que **ya sabemos** que una película es muy buena? Por este motivo considero muy acertado que se haya construido un set de testeo específico a fin de evaluar la precisión de la lista Top-N recomendada.

En la sección **2.1** se hace un comentario que me dejó muy asombrado, y es que en el caso de Netflix sólo 1.7% de las películas (más populares) acumulan más de un tercio de los ratings totales realizados por los usuarios. Esto significa que esas películas tienen demasiada influencia sobre las métricas RMSE y MAE. Dicho esto, y conectándolo con el párrafo anterior, lo que nos gustaría es recomendar películas no tan populares pues de otra manera nuestra recomendación no tiene valor para el usuario (simplemente bastaría usar *most popular*). Por este motivo considero muy buena idea haber separado el set de Test en dos partes (Long y Short Tail). 

En la sección de los experimentos se realizan comparaciones de distintos modelos según su capacidad de predecir si cierta película de rating 5 está dentro de la lista recomendada. Ahí se introduce un modelo de estilo factorización matricial que denominan PureSVD que es básicamente hallar la factorización en valores singulares de la matriz usuarios-items. Me parece que este modelo es muy adecuado para lo que quieren mostrar los investigadores pues precisamente no es un modelo derivado de minimizar algún tipo de error. 

En cuanto a los resultados, creo que la interpretación que dan los investigadores acerca de la mejoría del PureSVD para items no populares al incrementar la cantidad de factores es muy apresurada. Al respecto mencionan que al incrementar los factores el modelo puede "capturar" mejor las *features* de los items no populares, sin embargo, creo que esto no es fácil de deducir (y de hecho, no presentan justificación de eso). Da la impresión de que en este punto hay una especie de caja negra en que PureSVD entrega mejores listas de recomendaciones para items no populares a medida que aumentamos su complejidad, y es aquí donde surgen preguntas ¿Vale la pena seguir aumentando la complejidad del modelo?, y si es así ¿Hasta que punto y cual es el costo computacional de hacerlo?

Me impresionó que PureSVD sea el mejor modelo al considerar las listas Top-N, sin embargo, creo que también es necesario mostrar los errores en promedio como se hace típicamente pues esto da una idea de que tan distinto es considerar estos dos enfoques. Al no mostrar esa diferencia da la impresión de que sólo importa el enfoque que los investigadores proponen y no una real comparación para ver los contrastes.

Otra cosa que me parece excelente es que en la conclusión los investigadores discuten cómo se podría mejorar el PureSVD, por ejemplo, considerando llenar la matriz con valores distintos de 0. Una cosa que no se menciona es qué pasaría con PureSVD para recomendar a un usuario nuevo puesto que asignaría todos sus ratings como 0, creo razonable que se le asignen los items más populares pero no me queda claro si funcionaría así. 



