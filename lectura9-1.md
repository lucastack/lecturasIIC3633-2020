# Lectura: Multi-armed recommender system bandit ensembles. Cañamares, R., Redondo, M., & Castells, P. (2019).

El paper comienza introduciendo una nueva manera de hacer las recomendaciones. Típicamente, dado un conjunto de datos de ratings separado en un set de entrenamiento y testeo, nos interesa encontrar el mejor modelo para llevar a cabo las recomendaciones, una vez hecho esto el problema está resuelto. El anterior enfoque es estático pues el modelo ya quedó fijado y también así las recomendaciones, y en caso de tener nuevos datos tendríamos que correr los algoritmos desde el principio. Los investigadores presentan el problema de recomendación como uno dinámico: a medida que nuevos datos llegan el sistema recomendador los incorpora y genera nuevas recomendaciones para los usuarios.    

En este contexto, los autores proponen utilizar *multi armed bandits* para resolver el problema. La idea principal es generar ensambles de modelos y que cada brazo del *bandit* corresponda a un conjunto de parámetros que determina al ensamble. Dicho de manera más práctica, esto significa que a medida que se reciben nuevos datos el bandit entrega un nuevo ensamble (o sus nuevos parámetros) para realizar nuevas recomendaciones.

Con respecto al problema de recomendación en si, me parece que es mucho más realista la versión dinámica que la estática del problema, y es que en muchos contextos de producción o de industria las recomendaciones se deben actualizar en tiempo real. En este punto es importante que los modelos propuestos sean capaces de generar nuevas recomendaciones rápidamente, y es por eso que un modelo dinámico es más apropiado dado que no necesita arrancar desde 0. Además, dado que está bien respaldada la superioridad de los ensambles por sobre los modelos individuales, es natural pensar en un modelo dinámico como aquel ensamble que modifique sus parámetros, por este motivo, creo que el mayor aporte del paper consiste en cómo incorporar *bandits* en ese esquema. En particular, ellos proponen utilizar los bandits *Thompson sampling* y *ε-greedy* para los ensambles.

En la sección de experimentos se utilizan 3 algoritmos para generar los ensambles: kNN, Matrix Factorization y Most Popular. En cada iteración del ciclo, el bandit asociado escoge alguno de esos algoritmos para hacer la recomendación, luego incorpora el feedback del usuario actualizando los parámetros del bandit y el algoritmo utilizado. Además, se experimentó utilizar los algoritmos por si solos, sin bandit, mostrando la comparación con Most Popular, el cual simplemente va actualizando los items más populares a medida que recibe feedback. Como sabemos, en términos de precisión Most Popular es una buena referencia de comparación.  

El resultado que más capturó mi atención es el presentado en la Figura 2: como los *bandits* evolucionan su proporción de uso de los algoritmos. Al principio, el algoritmo predilecto para generar las recomendaciones es el Most Popular, y como mencionan los investigadores, esto permite recolectar una mayor cantidad de información al obtener feedback de los usuarios. Este incremento de los ratings es lo que posteriormente hace que Matrix Factorization sea más utilizado pues baja el sparcity de la matriz usuario-items. En ese mismo gráfico también puede observarse que a partir de un cierto punto las proporciones parecen estabilizarse, sin embargo, hay algo que no se explica ¿Por qué el traffic ratio del bandit *ε-greedy* posee esa forma discreta y muy estable, y no una curva como la del *Thompson sampling*?.

En cuanto a las conclusiones, a partir de los resultados y las figuras presentadas es claro que el esquema propuesto es superior a los otros con los que se compara, pero en esta misma sección se menciona que desde el punto de vista computacional, correr solo uno de estos ensambles que utilizan *bandits* es mejor que ejecutar muchos algoritmos cada vez, sin embargo, no se adjunta ningún resultado que justifique tal afirmación.

En general, considero es un paper muy denso en el sentido de que posee una extensión relativamente corta pero mucha información. Por eso mismo creo que los investigadores podrían haber destinado una subsección del paper para explicar un poco más acerca de cómo funcionan los bandits *Thompson sampling* y *ε-greedy* con la finalidad de contextualizar mejor al lector más inexperto en el tema.



